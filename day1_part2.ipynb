{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks (ANNs) for image data\n",
    "_by Phil Harrison (February 2021)_\n",
    "#### Dataset\n",
    "For this exercise we will use the MNIST dataset: greyscale images (28 x 28 pixels) of hand-written digits that we wish to classify into their corresponding ten categories (0 to 9). Note we now have a classification problem, as opposed to the QSAR regression problem from part 1 of today's lab. The MNIST dataset is a classical benchmark dataset in machine learning, a first port of call for testing new neural network architectures. It consists of 60,000 training images and 10,000 test images. It is like the 'Hello world' for neural networks. The images were collected by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. The images were centred and scaled to fit into the 28 x 28 pixel crops.\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/mnist.png\" alt=\"drawing\" style=\"width:800px;\"/>\n",
    "    <center>Figure 1. Example MNIST images (figure from Wikipedia).</center>\n",
    "</p>\n",
    "\n",
    "#### Modelling approach\n",
    "Here we will flatten the 28 x 28 pixel images into vectors of size 785 (= 28*28) and try to solve the classification problem with ANNs. For such a simple task with such small size images this will work quite well. As we move to larger images, with more image channels (such as red, green and blue (RGB) as in most colour images), such flattening and analysis with plain (vanilla) neural networks will not work. How to more adequately deal with more complex image data and classification problems will be the focus of the lectures and labs during the rest of this week. One point to note with respect to this flattening of the data is that we lose spatial information (think about it, each pixel in an image tends to be correlated with the pixels to the left, right, above and below it). How might we accomodate for this? Well, an extremely good approach is to use convolutional neural networks (CNNs, the focus of the lecture and lab tomorrow and the assignment you will do). For this lab we will overlook this complication and move forward with a flattened representation of the image data.\n",
    "\n",
    "#### References\n",
    "This section of the lab borrows/steals heavily from material from the book written by Francois Chollet (the invenor of Keras!; https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). This book is great! I highly recommed it. Some ideas I also stole from the Udemy course (https://www.udemy.com/course/deep-learning-tensorflow-2/), which was also quite good... Whilst I'm at it, for those who are interested, I highly recommend checking out Coursera (https://www.coursera.org/) and Udemy (https://www.udemy.com/), two online platforms where you can learn much more about AI, machine learning and deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers, datasets\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Helper libraries\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['train', 'valid'], loc='upper right')\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(model_history.history['accuracy'])\n",
    "    ax.plot(model_history.history['val_accuracy'])\n",
    "    ax.set(title=model_name + ': Model accuracy', ylabel='Accuracy', xlabel='Epoch')\n",
    "    ax.legend(['train', 'valid'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, model_name,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    title = model_name + ': Confusion Matrix'\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n",
    "We will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in the `[0, 1]` interval. Our training images were downloaded in an array of shape `(60000, 28, 28)` of type `uint8` with values in the `[0, 255]` interval. We will transform this into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1.\n",
    "\n",
    "As we will compare multiple models below it makes sense to split the training dataset up into a 'proper' training set and a validation set. The validations set we will use to decide on the best model. For a final evaluation we then use the test set, which we do not look at until the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "class_names = list(np.arange(10))\n",
    "\n",
    "n_train0 = 60000\n",
    "X = train_images.reshape((n_train0, 28 * 28))\n",
    "X = X.astype('float32') / 255\n",
    "\n",
    "random.seed(4826)\n",
    "indices = np.arange(n_train0)\n",
    "random.shuffle(indices)\n",
    "\n",
    "n_train = int(n_train0 * 0.8) # 80% of data for (proper) training and 20% for validation\n",
    "X_train, X_valid = X[indices[:n_train]], X[indices[n_train:]]\n",
    "\n",
    "X_test = test_images.reshape((10000, 28 * 28))\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(train_labels[indices[:n_train]])\n",
    "y_valid = to_categorical(train_labels[indices[n_train:]])\n",
    "y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pass model\n",
    "A quote from Francois Chollet to get us started:\n",
    "\n",
    "\"The core building block of neural networks is the \"layer\", a data-processing module which you can conceive as a \"filter\" for data. Some data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers which will implement a form of progressive \"data distillation\". A deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters -- the \"layers\".\"\n",
    "\n",
    "Note below we have a mutliclasss classifcation problem so we need to use a softmax activation for the final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and summarise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = layers.Input(shape=X_train[0].shape)\n",
    "x = layers.Dense(32, activation='relu')(inps)\n",
    "preds = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "ANN1 = models.Model(inputs=inps, outputs=preds)\n",
    "ANN1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a classification problem we need to use a different loss function than the MSE we used for regression. In this case we shall use categorical crossentropy. For classification tasks we can also monitor the accuracy of our predictions as the model trains across the epochs. The loss and metrics to keep track of are specified when we compile the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, fit and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "n_epochs = 10\n",
    "\n",
    "ANN1.compile(optimizer=optimizers.RMSprop(lr=lr), \n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = ANN1.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=n_epochs, verbose=2)\n",
    "plot_history(history, 'ANN1')\n",
    "\n",
    "val_loss, val_acc = ANN1.evaluate(X_valid, y_valid, verbose=0)\n",
    "print('ANN1 on validation data: loss = ' + str(np.round(val_loss, 3)) + \n",
    "      '; acc = ' + str(np.round(val_acc, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore alternative models\n",
    "In the code cells below (generate more cells if you need them) you should define, compile and fit (at least two) more models and evaluate them on the validation data. You can explore making the model wider (with more neurons per layer) and deeper (with more layers). You could also explore adding dropout. It is often difficult in advance to say how wide and deep neural network architectures should be for a given dataset and hence some exploration, some trial and error, is often required. You can also change the number of epochs to train for if it looks like your validation loss still wants to go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and summarise your ANN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile, fit and evaluate your ANN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and summarise your ANN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile, fit and evaluate your ANN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation on test data\n",
    "My best model on the validation data was the third one I tried (ANN3). In the code cell below change all the 'ANN3's to the name you gave your best model above and run the cell to see your final test loss, accuracy and a confusion matrix for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = ANN3.evaluate(X_test, y_test, verbose=0)\n",
    "print('ANN3 on test data: loss = ' + str(np.round(test_loss, 3)) +\n",
    "     '; acc = ' + str(np.round(test_acc, 3)))\n",
    "\n",
    "y_pred = ANN3.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test.argmax(axis=-1), y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(20,7), facecolor='w')\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, model_name='ANN3 on test data')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look to the confusion matrix above. Are the highest numbers for those misclassified for the test data (i.e. those on the off-diagonals) the numbers you might expected would be easier to mix up when based on different people's hand-writing? \n",
    "\n",
    "Running the code cell below will plot a randomly chosen set of four examples where the predictions were wrong. You can run it multiple times to see different examples. Some of the mistakes are obvious, others are less so. Those that are less so could be due, for example, to a somewhat off average slant in how the digits were written. Afterall, flattening the images as we did looks at each pixel in isolation. If we can take the correlated nature of image pixels into account we can do much better. The best way of doing this is to use convolutional neural networks (CNNs), the focus of the rest of this week. With CNNs on the MNIST data we can easily get greater than 99% accuracy on the MNIST test images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_vec = y_test.argmax(axis=-1)\n",
    "mis_indices = np.where(y_test_vec != y_pred)[0]\n",
    "mis_samp = np.random.choice(mis_indices, 4)\n",
    "fig = plt.figure(figsize=(20, 10), facecolor='w')\n",
    "for i in range(4):\n",
    "    sub_index = 241 + i\n",
    "    plt.subplot(sub_index)\n",
    "    plt.imshow(test_images[mis_samp[i]], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('true = ' + str(y_test_vec[mis_samp[i]]) + \n",
    "             '; pred = ' + str(y_pred[mis_samp[i]]))\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
