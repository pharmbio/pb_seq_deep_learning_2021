{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'markdown cell' below  replace the `???` with the names of those in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment (part 1): classification of cell morphological changes with LeNet\n",
    "_by Phil Harrison (February 2021)_\n",
    "#### Inspiration\n",
    "This step by step exercise was inspired by Andrew Ng's **Deep learning** Coursera specialization (https://www.coursera.org/specializations/deep-learning) . This assignment will take you through a machine learning workflow which includes reading datasets, creating and compiling models, training models on datasets, and predicting on the validation sets (for model comparisons). Much of the experience you accumualted during the labs from yesterday and the day before will come in useful here.\n",
    "\n",
    "#### Datasets\n",
    "The specific dataset we will use is a subset of the bbbc021v1 MoA dataset (http://mct.aacrjournals.org/content/9/6/1913) available from the Broad Bioimage Benchmark Collection (https://www.nature.com/articles/nmeth.2083). We chose to only use a subset of this data (based on only the six main MoAs) in order that the models we will fit below would not take too long to train.\n",
    "\n",
    "#### Importance\n",
    "After you've completed this assignment you will know how to develop and utilize advanced machine learning models (in this case convlutional neural networks (CNNs) applied to high content cell images). Traditional approaches for classyfing such biological cell-images involve complex workflows, with many steps requiring manual implementation. The more modern neural network approach (made possible through the better hardware available today, most notably via GPUs) can perform equally well and often better than the traditional approaches. What's more these CNNs, based merely on the pixel-intensities of the images, require significantly less domain expertise.\n",
    "\n",
    "#### Note 1\n",
    "Using the entire MoA dataset, transfer learning and data augmentation is a preferable way to more fully explore this data (as was done in the paper written by Alexander Kensert - a former student from the same PB-seq masters course you are now on! (https://journals.sagepub.com/doi/10.1177/2472555218818756)). This paper was discussed during the lectures.\n",
    "\n",
    "Adding transfer learning to the models will be explored in part 3 of the assignemt. This third part should be attempted by those of you that would like to attain a **VG mark** for the assignment. To attain a **G mark** only parts 1 and 2 need be completed.\n",
    "\n",
    "#### Note 2\n",
    "For the assignment we will split the data into a training and validation set and will only optimize our performance on the validation set. Although from the work with the oral cancer data you saw that it is genrally best to keep out a test set on which to make a final evaluation of your chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Helper libraries\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Don't worry too much about the code in the functions below, but you might want to go through when they are called later on so that you roughly understand what they're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dirname = 'bbbc021v1_images'\n",
    "    x_orig = np.zeros((660, 256, 256, 3), dtype=np.float32)\n",
    "\n",
    "    for f in range(x_orig.shape[0]):\n",
    "        img    = Image.open(dirname + '/bbbc021v1_%s.png' % str(f))\n",
    "        img    = np.array(img)\n",
    "        x_orig[f] = img\n",
    "\n",
    "    labels = pd.read_csv('bbbc021v1_labels.csv',\n",
    "                          usecols=[\"compound\", \"concentration\", \"moa\"],\n",
    "                          sep=\";\")\n",
    "    y_orig = np.array(labels['moa'])\n",
    "\n",
    "    return x_orig, y_orig\n",
    "\n",
    "def convert_to_one_hot(y, C):\n",
    "    moa_dict = {'Aurora kinase inhibitors': 0, 'Cholesterol-lowering': 1,\n",
    "                'Eg5 inhibitors': 2, 'Protein synthesis': 3, 'DNA replication': 4, 'DNA damage': 5}\n",
    "\n",
    "    y = np.asarray([moa_dict[item] for item in y])\n",
    "    y = np.eye(C)[y]\n",
    "    y = y.astype('float32')\n",
    "\n",
    "    return y\n",
    "\n",
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    ax = fig.add_subplot(131)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['train', 'valid'], loc='upper right')\n",
    "    \n",
    "    ax = fig.add_subplot(132)\n",
    "    ax.plot(np.log(model_history.history['loss']))\n",
    "    ax.plot(np.log(model_history.history['val_loss']))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')    \n",
    "\n",
    "    ax = fig.add_subplot(133)\n",
    "    ax.plot(model_history.history['accuracy'])\n",
    "    ax.plot(model_history.history['val_accuracy'])\n",
    "    ax.set(title=model_name + ': Model accuracy', ylabel='Accuracy', xlabel='Epoch')\n",
    "    ax.legend(['train', 'valid'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, model_name,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    title = model_name + ': Confusion Matrix'\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def valid_evaluate(model, model_name):\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_pred = y_pred.argmax(axis=-1)\n",
    "    y_true = Y_valid.argmax(axis=-1)\n",
    "    \n",
    "    class_names = ['Aur', 'Ch', 'Eg5', 'PS', 'DR', 'DS']\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure(figsize=(15,5), facecolor='w')\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, model_name=model_name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print('')\n",
    "    print('classification report for validation data:')\n",
    "    print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, y_orig = load_dataset()\n",
    "\n",
    "print(\"Shape of X_orig = \" + str(X_orig.shape))\n",
    "print(\"Shape of y_orig = \" + str(y_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding of targets and standardization of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = convert_to_one_hot(y_orig, 6)\n",
    "X = X_orig/255.\n",
    "\n",
    "print(\"Shape of Y_orig = \" + str(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets\n",
    "We will use 500 of the images for training and the remaining 160 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 500\n",
    "\n",
    "random.seed(5026)\n",
    "indices = np.arange(len(Y))\n",
    "random.shuffle(indices)\n",
    "\n",
    "X_train, X_valid = X[indices[:n_train]], X[indices[n_train:]]\n",
    "Y_train, Y_valid = Y[indices[:n_train]], Y[indices[n_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot samples of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to plot few example images of the training set\n",
    "fig, ax = plt.subplots(2, 3, figsize=(14, 10))\n",
    "fig.suptitle(\"Examples of training images\", fontsize=20)\n",
    "axes = ax.ravel()\n",
    "for i in range(len(axes)):\n",
    "    idx = np.where(np.argmax(Y_train, axis=1) == i)[0]\n",
    "    s_idx = np.random.choice(idx)\n",
    "    img = (X_train[s_idx]*255).astype(\"uint8\")\n",
    "    axes[i].set_title(y_orig[indices][s_idx], fontsize=14)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_axis_off()\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (LeNet inspired)\n",
    "\n",
    "In this lab you will learn how to implement your own CNN for classifying cell phenotypes. You will be using the open-source machine learning framework TensorFlow (https://www.tensorflow.org) to do so.\n",
    "\n",
    "Convolutional Neural Networks (CNNs) use convolutions instead of the normal fully connected layers, which have proven to be highly successful for image recognition tasks. By convolving filters on the input layer and outputting the results to the next layer, the CNN \"detects\" (or learns) features at different levels of abstraction throughout the network. With lower-level abstractions (like edges and blobs) in the early layers, and higher-level abstractions (like cells) in deeper layers. Figure 1 shows the LeNet inspired CNN that we will shortly turn into TensorFlow code. It has 3 convolutional layers, 3 max pooling layers, and two final fully connected layers.\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/conv.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 1. LeNet inspired CNN that you will implement.</center>\n",
    "</p>\n",
    "\n",
    "\n",
    "If you're getting stuck or stumble upon problems, don't hesitate to use Google or similar to search for answers. There's plenty of information out there regarding TensorFlow (and Keras - the default language of Tensorflow 2) implementations. Good places to look:\n",
    "* https://www.tensorflow.org\n",
    "* https://keras.io \n",
    "* https://github.com/keras-team/keras/tree/master/examples \n",
    "\n",
    "Finally, make use of the lecture notes and previous labs and try to think carefully about what you're actually implementing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Coding LeNet\n",
    "Replace the `???` parts in the cell below with your own code. Start by defining the input shape and then try to code in the variant of LeNet shown above in Figure 1. The final line of your code should be a dense layer with a softmax activation. Once you have your code ready run the cell and see if your model summary matches ours (see below). Note, the stride of 2 for the first convolutional layer is something you should define within the braces of `layers.Conv2D(...)` along with the other arguments you are now used to. You can press `STIFT+TAB` from inside the braces to see the aruments that can be passed. Also pay attention to how the dimensions above change from one part of the model to the next - with the max pooling we don't always use the defualt pool size of `(2, 2)`. To compensate for over-fitting add a dropout layer prior to your prediction layer with a dropout rate of `0.2`. Name your model `LeNet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and sumarise the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you define LeNet correctly?\n",
    "At the end of the summary you should see the following:\n",
    "\n",
    "Total params: 1,121,062\n",
    "\n",
    "Trainable params: 1,121,062\n",
    "\n",
    "Non-trainable params: 0\n",
    "\n",
    "### Question:\n",
    "In the first layer you will see in the number of parameters/weights to estimate/train is 1216. How is this number calculated? Replace the `???` in the 'markdown' cell below with your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "LeNet.compile(optimizer=optimizers.Adam(lr=lr), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate the LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 30\n",
    "\n",
    "history = LeNet.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs,\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    verbose=2)\n",
    "\n",
    "plot_history(history, 'LeNet')\n",
    "valid_evaluate(LeNet, 'LeNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple runs of the same model\n",
    "Re-running the model above, due to stochasticity in the local minima that the model achieves, will give different results. To get a more robust assessment of our model re-run it 5 times and compute the mean of the 'weighted avg f1-score' from the classification report. In the code cell below write each of these 5 values and compute their mean.\n",
    "\n",
    "To re-run the model properly you need to rerun the model definition, model compiling and model fitting code cells above.\n",
    "\n",
    "When I ran this model five times I got the following result for the mean 'weighted avg f1-score':\n",
    "\n",
    "(0.707 + 0.774 + 0.723 + 0.834 + 0.797) / 5  = 0.767"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
