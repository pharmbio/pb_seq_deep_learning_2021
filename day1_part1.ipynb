{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks for QSAR\n",
    "_by Phil Harrison (February 2021)_\n",
    "#### Dataset\n",
    "For this exercise we will use the same dataset of aqueous solubility of 1142 diverse chemical compounds as you previously explored during the QSAR lab last week. However, here we will only use the PhysChem descriptors.\n",
    "\n",
    "#### Modelling comparisons\n",
    "1. Compare the results of linear regression to those of a simple neural network with no hidden layers and no non-linear activation functions\n",
    "2. Compare the results of a a random forest regressor, a support vector regressor, and a neural network with two hidden layers (with non-linear activations) and dropout.\n",
    "\n",
    "#### Aims\n",
    "* to see the link between neural networks and linear regression\n",
    "* to learn the basics of how to define, compile, fit and evaluate neural networks via TensorFlow.\n",
    "\n",
    "#### Note\n",
    "We will be using the open-source machine learning framework TensorFlow (https://www.tensorflow.org) and Keras (https://keras.io) for our neural networks. TensorFlow was developed by the Google Brain team and is today one of the most widely used machine learning frameworks in research and industry and Keras was/is the most popular higher-level API that runs atop TensorFlow. However, last year TensorFlow 2 was released. In TensorFow 2 (which we will use for all our neural network work) Keras is now fully integrated. This means that we get all the benefits of TensorFlow with a much easier (Keras-type) way to define and train models than was previously possible with TensorFlow 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers, datasets\n",
    "\n",
    "# Helper libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(15,5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(np.log(model_history.history['loss']))\n",
    "    ax.plot(np.log(model_history.history['val_loss']))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and check shape of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('data/X_qsar.npy')\n",
    "y = np.load('data/y_qsar.npy')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test sets and standardize the data\n",
    "Here we will just have a training and test set, so our results will not be quite as rigerous as those you got with cross-validation in the supervised machine learning lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(y) * 0.7) # 70% of data for training and 30% for testing\n",
    "\n",
    "random.seed(1234)\n",
    "indices = np.arange(len(y))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# X_train0 is our training data prior to standardization\n",
    "X_train0, X_test0 = X[indices[:n_train]], X[indices[n_train:]]\n",
    "y_train, y_test = y[indices[:n_train]], y[indices[n_train:]]\n",
    "\n",
    "# standardize X_train0 and X_test0 to give X_train and X_test\n",
    "scaler = StandardScaler().fit(X_train0)\n",
    "X_train = scaler.transform(X_train0)\n",
    "X_test = scaler.transform(X_test0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X_train, y_train)\n",
    "LR_pred = LR_model.predict(X_test)\n",
    "LR_mse = mean_squared_error(y_test, LR_pred)\n",
    "print('Linear Regression: MSE = ' + str(np.round(LR_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical neural network as a linear regression\n",
    "If we define a neural network with no hidden layers and no non-linear activations we essentailly get the same results as we do with basic linear regression. The results below should help clarify that to you (there are some minor differences hovever, hence the MSE for the neural network will not be _exactly_ the same as the results above for linear regression, but they are neverthelss very close).\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/lin-reg.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 1. Our neural network version of linear regression.</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and summarise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = layers.Input(shape=X_train[0].shape)\n",
    "preds = layers.Dense(1)(inps)\n",
    "\n",
    "ANN1 = models.Model(inputs=inps, outputs=preds)\n",
    "ANN1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model\n",
    "The learning rate and optimizer chosen below are both things that can be changed when one explores hyper parameter options, different architectures and what not. Below we use a learning rate (lr) of 0.001 (a common default learning rate) and the 'RMSprop' optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "ANN1.compile(optimizer=optimizers.RMSprop(lr=lr), loss='mse')\n",
    "\n",
    "history = ANN1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, verbose=0)\n",
    "plot_history(history, 'ANN1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN1_mse = ANN1.evaluate(X_test, y_test, verbose=0)\n",
    "print('ANN1: MSE = ' + str(np.round(ANN1_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor & Support Vector Regressor\n",
    "For comparative purposes, with the results we will explore later with a more involved neural network architectures than the one above, we will build a random forest and support vector model (perhaps some of you already did this at the end of the supervised machine learning lab?). For these two machine learning algorithms we will just use the default hyper parameter settings, which are often a good place to start. This means that you will just have () after the model definition, as you did for the linear regression with LinearRegression(). To change the hyper parameters from the defaults one needs to specify them within the braces (...).\n",
    "\n",
    "The code cells for the random forest and support vector regressors have been left blank below. You should fill in these cells. You should define the models, fit them, make predictions from them, compute their MSEs and print out the results.\n",
    "\n",
    "* hint 1: look to the cell where we 'Load packages' to get the right model definition for the two machine learning methods\n",
    "* hint 2: look at the cell where we do 'Linear Regression'. The code should be pretty darn similar to this :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper with ANNs \n",
    "In the cells below we define, compile, fit and evaluate a neural network model with:\n",
    "* two hiiden layers, each with 32 neurons and non-linear activations (relu)\n",
    "* a dropout layer at the end with a dropout rate of 0.2\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/relu-activation.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "    <center>Figure 2. relu activation.</center>\n",
    "</p>\n",
    "\n",
    "Dropout can help to avoid overfitting, much as L1 and L2 regularizations do (as you explored in the supervise machine learning lab). In the model loss plots (below) this stops the test loss from increasing as you train for more epochs.\n",
    "\n",
    "Some quotes from a paper I co-authored called \"Deep Learning in Image Cytometry: A Review\" (https://onlinelibrary.wiley.com/doi/full/10.1002/cyto.a.23701):\n",
    "\n",
    "\"_Overfitting occurs when the parameters of a model fit too closely to the input training data, without capturing the underlying distribution, and thus reducing the model’s ability to generalize to other datasets_\".\n",
    "\n",
    "DROPOUT: \"_A regularization technique that reduces the interdependent learning among the neurons to prevent overfitting. Some neurons are randomly “dropped,” or disconnected from other neurons, at every training iteration, removing their influence on the optimization of the other neurons. Dropout creates a sparse network composed of several networks—each trained with a subset of the neurons. This transformation into an ensemble of networks hugely decreases the possibility of overfitting, and can lead to better generalization and increased accuracy_\".\n",
    "\n",
    "<p>\n",
    "    <img src=\"figs/dropout.png\" alt=\"drawing\" style=\"width:1200px;\"/>\n",
    "    <center>Figure 3. Dropout.</center>\n",
    "</p>\n",
    "\n",
    "In what comes below there are no missing cells or code lines for you to fill in, this is simply an example. But pay attention to how the code is written below as in the jupyter notebook 'day1_part2' there will be missing parts that you will have to fill in. The code below will help you with those later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = layers.Input(shape=X_train[0].shape)\n",
    "x = layers.Dense(32, activation='relu')(inps)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "preds = layers.Dense(1)(x)\n",
    "\n",
    "ANN2 = models.Model(inputs=inps, outputs=preds)\n",
    "ANN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "ANN2.compile(optimizer=optimizers.RMSprop(lr=lr), loss='mse')\n",
    "\n",
    "history = ANN2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, verbose=0)\n",
    "plot_history(history, 'ANN2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN2_mse = ANN2.evaluate(X_test, y_test, verbose=0)\n",
    "print('ANN2: MSE = ' + str(np.round(ANN2_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So...\n",
    "above you should have gotten the lowest MSE for the neural network, followed by the random forest and then the support vector regressor. These results are however not the final say. Different hyper parameter settings for any of these machine learning algorithms could change the rankings. For neural networks there are many hyper parameters that one could explore, including the network architecture, the number of layers, the number of neurons per layer, the drop out rate(s), the learning rate and the optimizer to use. In later labs this week we will explore these, and additional, choices. A full comparison would also better be done via cross validation as our results above are also affected by the train/test splitting of the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.S.\n",
    "The TensorFlow webiste (https://www.tensorflow.org) is a great place to look if you're interested in exploring neural networks further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
